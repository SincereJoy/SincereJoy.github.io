---
title: MIT 6.824-Lab1-MapReduce
key: 12
author: ZE
tags: 分布式系统 lab MIT6.824
---
[Lab1官方指南](https://pdos.csail.mit.edu/6.824/labs/lab-mr.html)

[MapReduce paper](http://research.google.com/archive/mapreduce-osdi04.pdf)

<!--more-->
## 概览
Lab1的任务是实现一个分布式的MapReduce程序，包括一个corrdinator（论文中叫Master）和多个并行的worker，corrdinator和workers之间通过RPC进行通信。

![Overview](/assets/images/6.824-lab1-overview.jpg)

Entity：
* corrdinator：负责给workers分发任务，并对一定时间内未完成的任务进行处理
* workers：负责执行corrdinator给的任务

Task：
* Map: 把输入文件中的kv数据根据key的哈希值映射到不同的中间文件中
* Reduce：累计中间文件中key值相同的value值，写入最终的结果文件

![MapReduce示意图](/assets/images/6.824-mr.jpg)

## 实现细节
### 网络通信
Map()读取的是本地的输入文件，不需要通过网络传输源数据文件；Reduce()需要通过网络读取运行其他worker的服务器的中间结果。由于中间结果文件数远小于所有键数，所以MapReduce的网络通信是很高效的。在实际场景中workers应该部署在不同的服务器上，但Lab1只要求在一台机器上模拟。

### 如何实现多个worker的负载均衡？
**每个worker执行完当前的任务后，会再次向coordinator请求新的任务**，这样执行快的worker会处理更多的任务，慢的处理更少的，避免出现多个执行完成的服务器等待一个慢服务器的情况。

### 如何实现容错？
1. coordinator会先分发Map任务，等所有Map任务都完成后再分发Reduce任务，出错的Map任务被处理后不会影响到Reduce任务
2. 各个worker是并行的，但每个worker只能同时处理一个任务，即worker内部是串行的，保证了各Map任务以及各Reduce任务之间没有交互。

因此**当某个任务执行出错时，只需要重新运行这个任务**就可以了，出错的任务不会影响其它任务的运行结果。

### 如何处理coordinator把同一个Map任务分配给两个worker执行的情况？
导致这种情况的原因：
coordinator先把map task1分配给了worker1，但worker1在限制时间内没有返回结果，此时coordinator会默认worker1 crash了，因此会把task1分配给worker2执行，但worker1可能并没有崩溃，只是执行比较慢，这就会导致一段时间后worker1和worker2都返回了task1的执行的结果。

造成的问题：
不确定Reduce任务该去哪个worker中读取对应的中间文件

解决方案：
coordinator记录分配出去的每个任务对应的worker，当收到其它worker返回的该任务的结果时，直接忽略。仅通知Reduce任务去记录的worker那里读取数据。

### 如何处理coordinator把同一个Reduce任务分配给两个worker执行的情况？
导致这种情况的原因：
跟上述问题类似

造成的问题：
多个Reduce worker同时写同一个文件（输出文件是根据Reduce任务的序号命名的， mr-out-X）

解决方案：
每个worker执行Reduce任务时，先把结果写入一个临时文件，写完后再利用**重命名原子操作**把临时文件重命名

## 具体实现
### rpc.go
定义coordinator和worker进行RPC通信的结构，需要两对request-response结构，一对用于worker向coordinator请求任务，一对用于worker告知coordinator任务完成情况。

```go
// Request
type TaskRequestArgs struct {
	WorkerId int // 便于coordinator记录任务分配给了哪个worker
}

// Response
type Task struct {
	TaskId int 
	Status int // cooridnator当前的状态
	TaskNum int // nReduce or nMap
	FilePath string // Map任务要读取的输入文件名 
}

// Request
type WorkerReply struct {
	Task *Task
	WorkerId int
	Finished bool
}

// Response
type Response struct {
	Succeed bool
}
```

其中，Task中的TaskNum对于Map task来说是nReduce，对于Reduce task来说是nMap。

Task中的Status记录的是coordinator当前的状态，不需要额外记录每个Task的状态，因为coordinator的状态可以反映出Task的状态。

```go
// coordinator.go
// Define status
const (
	MapTask = iota
	ReduceTask
	Wait
	Complete
)
```
当coordinator状态是MapTask时，它分发出去的任务都是Map任务；类似地，当coordinator状态是ReduceTask时，它分发出去的任务都是Reduce任务。当coordinator处于Wait状态时，说明任务队列为空，在等待其他worker执行完毕再进入下一个状态，此时没有分配到任务的worker也进行等待。当coordinator状态时Complete时，说明所有任务都完成了，workers可以直接退出。

另外还需要注意，GO的RPC call只传输首字母大写的字段（导出字段）

### worker.go
在Worker函数中实现worker的完整逻辑（包括Map和Reduce函数），另外还需要实现两个RPC call函数（参考CallExample()）

其中Worker的执行逻辑是：
1. 初始化（创建随机的workerId）
2. 向Coordinator请求任务
3. 根据获得任务的Status执行不同的操作（上一节提到了不同Status下worker需要执行的操作）
4. 执行完任务后向Coordinator发送任务执行情况
5. 等待几秒后再次请求任务（回到第2步）

**Map函数**：
1. 读文件
2. 构建KeyValue map
3. 写入中间文件（先写入临时文件，再重命名）

第1步直接参考main/mrsequential.go的实现

第2步先调用mapf把所有键值对读入到一个kv列表里，然后遍历这个列表，根据key的哈希值（ihash(kv.Key)%nReduce）把kv放入不同的中间文件中

第3步写中间文件，直接用[Lab1官方指南](https://pdos.csail.mit.edu/6.824/labs/lab-mr.html)里给出的读写json文件的代码。每个Map task需要写nReduce个中间文件，文件名是mr-maptaskid-reduceid


**Reduce函数**：
1. 读中间文件
2. 维护一个key-[]values map，对于每个key，把所有中间文件中这个key对应的value值都存入值列表中
3. 对上述map进行reduce操作reducef(key,values)，并保存为新的KeyValue
4. 排序
5. 写入输出文件

第1-3步的实现如下：

```go
    nMap := task.TaskNum
	intermediate := make([]KeyValue,0)
	kvMap := make(map[string][]string)

    // Read the intermediate file
	for i:=0; i<nMap; i++ {
		interFilename := fmt.Sprintf("mr-%v-%v", i, task.TaskId)
		file, err := os.Open(interFilename)
		if err!=nil {
			fmt.Printf("cannot open intermediate file %v",interFilename)
			return false
		}
		defer file.Close()

		dec := json.NewDecoder(file)
		for {
			var kv KeyValue
			if err := dec.Decode(&kv); err == io.EOF {
				break
			} else if err != nil {
				fmt.Printf("cannot decode intermediate file %v",interFilename)
			}
			kvMap[kv.Key] = append(kvMap[kv.Key], kv.Value)
		}
	}

	// Reduce
	for key, values:=range kvMap {
		intermediate = append(intermediate, KeyValue{ 
			Key: key, 
			Value: reducef(key, values)})
	}
```
第4、5步，排序和写入输出文件完全参考main/mrsequential.go的实现

### coordinator.go
coordinator.go的内容比较复杂，首先要**定义Coordinator结构体**（可以先把想到的字段写上，之后实现其它方法时需要其它的字段再补上），然后实现**初始化函数MakeCoordinator**和**终止函数Done**，还需要实现处理RPC call的**handler**。重点就是RPC call handler的实现，其它的都很简单。

**Coordinator的定义**
```go
type TaskWorkerMap struct {
	mp map[int]int
	mtx sync.RWMutex
}

type Coordinator struct {
	// Your definitions here.
	Status int 
	mutex sync.Mutex
	MapQueue chan *Task // 未处理的Map任务队列
	ReduceQueue chan *Task // 未处理的Reduce任务队列
	twMap TaskWorkerMap // task-worker映射
	nMap int    // Map任务总数
	nReduce int // Reduce任务总数
	nMapDone int // 已完成的Map任务数
	nReduceDone int // 已完成的Reduce任务数
}
```
加了两个互斥量用来实现线程同步，限制对coordinator状态和twMap的并发访问，执行test-mr.sh能够通过全部测试，但是在一处写coordinator状态和一处读twMap的地方还是会存在race，目前还没找到原因和解决方案。

twMap除了记录任务和worker的对应关系以外，还负责记录任务的状态，当任务还未被Map时，对应的worker设为-1，已Map还未Reduce时，worker设为-2，完成时worker设为-3。最开始实现corrdinator的时候没有这样做，发现超时处理内联函数中无法判断分配给worker的任务有没有完成，当时不想再额外维护一个存储任务完成状态的列表，所以给twMap添加了这个功能。

**MakeCoordinator和Done**

创建一个Coordinator c,赋初值，注意最开始只向MapQueue中添加任务，ReduceQueue是空的，等所有Map任务结束后再向ReduceQueue里添加任务

Done()函数的注释是：
> main/mrcoordinator.go calls Done() **periodically** to find out
> if the entire job has finished.
因此默认的返回值是false，当Coordinator的状态是Complete时，返回true。

**TaskHandler**

收到worker的任务请求，需要根据coordinator自身的状态给worker发送不同的回复（记为reply, 是一个Task）。

* Complete：只需要设置状态，告诉worker可以退出了 reply.status = Complete
* Wait: Wait状态下有六种情况
  1. MapQueue为空，还有map任务没有执行完成：继续等待
  2. ReduceQueue为空，还有reduce任务没有执行完成：继续等待
  3. 所有map任务已执行完成c.nMapDone == c.nMap：准备Reduce队列，状态变为ReduceTask
  4. 所有reduce任务已hi型完成c.nReduceDone == c.nReduce：状态变为Complete
  5. MapQueue不为空，有Map任务需要重新执行：状态变为MapTask
  6. ReduceQueue不为空，有Reduce任务需要重新执行：状态变为ReduceTask
* MapTask
  
  	![MapTaskHandler](/assets/images/6824-lab1-map.png)

* ReduceTask

	![ReduceTaskHandler](/assets/images/6824-lab1-reduce.png)

**ResponseHandler**

处理worker发送的任务完成情况，如果Reply.Finished == false，说明任务执行过程中遇到了错误，coordinator需要把未完成的任务加回到任务列表里；如果任务执行完成，则对应的任务完成数+1。twMap中保存的任务状态也需要做出相应的改变。

## 测试结果

![Test result](/assets/images/6824-lab1-result.jpg)