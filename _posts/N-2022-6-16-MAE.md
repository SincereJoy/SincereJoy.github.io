---
title: Masked Autoencoders Are Scalable Vision Learners
key: 6
author: ZE
tags: 论文阅读
---

论文链接：https://arxiv.org/pdf/2111.06377.pdf

code: https://github.com/facebookresearch/mae

<!--more-->

借鉴NLP的思想：masked language modeling，比如BERT，已经成为NLP领域常用的特征表示建模方法，对各种下游任务具有很好的泛化性。

MAE本质上是一种特殊的去噪自编码器（denoising autoencoder）

masked autoencoding在图像领域和自然语言领域的区别：

图像，自然信号，冗余信息